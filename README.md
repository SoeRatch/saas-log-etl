# SaaS Log ETL Pipeline with Apache Airflow

This project simulates an ETL (Extract, Transform, Load) pipeline for SaaS application logs using Apache Airflow, PostgreSQL, and Docker. The pipeline:

1. **Extracts** synthetic user log events (e.g., login, logout, purchase)
2. **Transforms** them into enriched, readable log entries
3. **Loads** them into a PostgreSQL database

---

## ğŸ› ï¸ Project Structure

```
project-root/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ log_etl_dag.py           # Main Airflow DAG
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_logs/                    # Raw JSONL logs
â”‚   â””â”€â”€ processed_logs/             # Transformed JSONL logs
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â””â”€â”€ log_generator.py         # Synthetic log generator
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â””â”€â”€ session_transformer.py   # Log enricher
â”‚   â””â”€â”€ load/
â”‚       â””â”€â”€ loader.py                # PostgreSQL loader
â”œâ”€â”€ docker-compose.yaml
â””â”€â”€ README.md
```

---

## âš™ï¸ Technologies Used

- ğŸŒ€ **Apache Airflow** â€” DAG scheduling and orchestration
- ğŸ³ **Docker Compose** â€” Local development and service containerization
- ğŸ˜ **PostgreSQL** â€” Structured data storage
- ğŸ **Python** â€” Custom ETL logic
- ğŸ“ **JSON Lines (JSONL)** â€” Log file format

---

## ğŸ§ª How the Pipeline Works

### â–¶ï¸ 1. Extraction (`log_generator.py`)

Generates synthetic SaaS log events (login/logout/purchase) in JSONL format. Logs are timestamped and saved like:

```
/opt/airflow/data/raw_logs/log_20250522_081657.jsonl
```

Each DAG run creates a new log file using UTC time to avoid overwriting.

---

### ğŸ”„ 2. Transformation (`session_transformer.py`)

Parses and enriches raw log events into a structured format:

- Assigns a log level (`info`, `debug`, `critical`)
- Adds human-readable messages
- Preserves metadata: `timestamp`, `user_id`, `session_id`

Example output:

```json
{
  "timestamp": "2025-05-22T08:10:11.411599+00:00",
  "level": "info",
  "message": "User user_17 logged out",
  "user_id": "user_17",
  "session_id": "session_5361"
}
```

---

### ğŸ§© 3. Load (`loader.py`)

- Reads the transformed logs
- Ensures the PostgreSQL table `processed_logs` exists
- Inserts all entries into the database

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/SoeRatch/saas-log-etl.git
cd saas-log-etl
```

### 2. Start Docker Services

```bash
docker-compose up --build
```

### 3. Initialize Airflow (first time only)

```bash
docker compose exec airflow-webserver airflow db init

docker compose exec airflow-webserver airflow users create \
  --username admin \
  --firstname First \
  --lastname Last \
  --role Admin \
  --email admin@example.com \
  --password admin
```

Access Airflow at: [http://localhost:8080](http://localhost:8080)

---

## ğŸ§ª Triggering the ETL Pipeline

1. Navigate to the Airflow UI
2. Enable the `log_etl_pipeline` DAG
3. Trigger it manually (calendar icon)
4. Watch task logs and validate success

---

## ğŸ—„ï¸ Verifying the Loaded Data

Connect to the PostgreSQL DB using a tool like **DBeaver**:

- Host: `localhost`
- Port: `5432`
- Database: `airflow`
- User: `airflow`
- Password: `airflow`

Check the `public.processed_logs` table under the `airflow` schema.

---

## âœ… PostgreSQL Table Schema

```sql
CREATE TABLE IF NOT EXISTS processed_logs (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ,
    level VARCHAR(20),
    message TEXT,
    user_id VARCHAR(50),
    session_id VARCHAR(50)
);
```

---

## âœ… Status

- âœ… Extract step complete (fake SaaS logs written daily)
- âœ… Transform step complete (log level + message enrichment)
- âœ… Load step complete (PostgreSQL insert + schema creation)

---

## ğŸ“¦ Future Improvements

- Add data validation & schema enforcement
- Add automated tests for each ETL step
- Introduce partitioning/indexing for faster queries
- Parameterize configs using `.env` or Airflow Variables
- Integrate with cloud data warehouse (e.g., BigQuery, Redshift)

---

## ğŸ‘¨â€ğŸ’» Author

[SoeRatch](https://github.com/SoeRatch)

---

## ğŸ“œ License

MIT License
